{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_version = 'bert-base-multilingual-cased'\n",
    "model = BertForMaskedLM.from_pretrained(model_version)\n",
    "model.eval()\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    model = model.cuda(0)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
    "\n",
    "def untokenize_batch(batch):\n",
    "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
    "\n",
    "def detokenize(sent):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    for i, tok in enumerate(sent):\n",
    "        if tok.startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
    "        else:\n",
    "            new_sent.append(tok)\n",
    "    return new_sent\n",
    "\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "MASK = '[MASK]'\n",
    "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
    "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
    "    \"\"\" Generate a word from from out[gen_idx]\n",
    "    \n",
    "    args:\n",
    "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
    "        - gen_idx (int): location for which to generate for\n",
    "        - top_k (int): if >0, only sample from the top k most probable words\n",
    "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
    "    \"\"\"\n",
    "    logits = out[:, gen_idx]\n",
    "    if temperature is not None:\n",
    "        logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
    "    elif sample:\n",
    "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        idx = dist.sample().squeeze(-1)\n",
    "    else:\n",
    "        idx = torch.argmax(logits, dim=-1)\n",
    "    return idx.tolist() if return_list else idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generation modes as functions\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
    "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
    "    seed_copy = seed_text.copy()\n",
    "    batch = []\n",
    "    for i in range(batch_size):\n",
    "        text = [MASK] * max_len\n",
    "        \n",
    "        for word in seed_text:\n",
    "            text[random.randint(0, len(text) - 1)] = word\n",
    "        text.append(SEP)\n",
    "        batch.append(text)\n",
    "    print(batch)\n",
    "    #if rand_init:\n",
    "    #    for ii in range(max_len):\n",
    "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
    "    return tokenize_batch(batch)\n",
    "\n",
    "def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
    "                                   cuda=False, print_every=10, verbose=True):\n",
    "    \"\"\" Generate for one random position at a timestep\n",
    "    \n",
    "    args:\n",
    "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
    "    \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    seed_ids = tokenizer.convert_tokens_to_ids(seed_text)\n",
    "    for ii in range(max_iter):\n",
    "        kk = np.random.randint(0, max_len)\n",
    "        if batch[0][kk] in seed_ids:\n",
    "            continue\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][kk] = mask_id\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        topk = top_k if (ii >= burnin) else 0\n",
    "        idxs = generate_step(out, gen_idx=kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
    "        if idxs is not list:\n",
    "            idxs = [idxs]\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][kk] = idxs[jj]\n",
    "        #if random.choice(seed_text) in batch[jj]:\n",
    "        #    break\n",
    "        if verbose and np.mod(ii+1, print_every) == 0:\n",
    "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
    "            for_print = for_print[:kk+1] + ['(*)'] + for_print[kk+1:]\n",
    "            print(\"iter\", ii+1, \" \".join(for_print))\n",
    "            \n",
    "    return untokenize_batch(batch)\n",
    "\n",
    "def parallel_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
    "                        cuda=False, print_every=10, verbose=True):\n",
    "    \"\"\" Generate for all positions at a time step \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        for kk in range(max_len):\n",
    "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, temperature=temperature, sample=sample)\n",
    "            for jj in range(batch_size):\n",
    "                batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and np.mod(ii, print_every) == 0:\n",
    "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
    "    \n",
    "    return untokenize_batch(batch)\n",
    "            \n",
    "def sequential_generation(seed_text, batch_size=2, max_len=15, leed_out_len=15, \n",
    "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
    "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_len):\n",
    "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+ii] = idxs[jj]\n",
    "        \n",
    "        return untokenize_batch(batch)\n",
    "\n",
    "\n",
    "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
    "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
    "             cuda=False, print_every=1):\n",
    "    # main generation function to call\n",
    "    sentences = []\n",
    "    n_batches = math.ceil(n_samples / batch_size)\n",
    "    start_time = time.time()\n",
    "    for batch_n in range(n_batches):\n",
    "        batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n",
    "                                               temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
    "                                               cuda=cuda, verbose=False)\n",
    "        \n",
    "        #batch = sequential_generation(seed_text,\n",
    "        #                              batch_size=batch_size, \n",
    "        #                              max_len=max_len, \n",
    "        #                              top_k=top_k,\n",
    "        #                              temperature=temperature, \n",
    "        #                              leed_out_len=leed_out_len, \n",
    "        #                              sample=False, cuda=cuda)\n",
    "        #batch = parallel_generation(seed_text, \n",
    "        #                            max_len=max_len, \n",
    "        #                            top_k=top_k,\n",
    "        #                            temperature=temperature, \n",
    "        #                            sample=sample, \n",
    "        #                            verbose=False,\n",
    "        #                            cuda=cuda,\n",
    "        #                            max_iter=max_iter)\n",
    "        \n",
    "        if (batch_n + 1) % print_every == 0:\n",
    "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "        \n",
    "        sentences += batch\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def printer(sent, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize(sent)[1:-1]\n",
    "    print(\" \".join(sent))\n",
    "    \n",
    "def read_sents(in_file, should_detokenize=False):\n",
    "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
    "    if should_detokenize:\n",
    "        sents = [detokenize(sent) for sent in sents]\n",
    "    return sents\n",
    "\n",
    "def write_sents(out_file, sents, should_detokenize=False):\n",
    "    with open(out_file, \"w\") as out_fh:\n",
    "        for sent in sents:\n",
    "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
    "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "batch_size = 1\n",
    "max_len = 5\n",
    "top_k = 1000\n",
    "temperature = 0.1\n",
    "\n",
    "leed_out_len = 15 # max_len\n",
    "burnin = 1000\n",
    "sample = True\n",
    "max_iter = 2000\n",
    "\n",
    "# Choose the prefix context\n",
    "seed_text = \"我 什 么 \".split()\n",
    "\n",
    "for temp in [temperature]:\n",
    "    bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
    "                          sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
    "                          cuda=True)\n",
    "    out_file = \"data/%s-len%d-burnin%d-topk%d-temp%.3f.txt\" % (model_version, max_len, burnin, top_k, temp)\n",
    "    write_sents(out_file, bert_sents, should_detokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_file = \"data/generations-len20-burnin200-temp0.700.txt\"\n",
    "bert_sents = read_sents(in_file, should_detokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    printer(sents[i], should_detokenize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score as bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar are the generated sentences to the original training data (Toronto Book Corpus and Wikipedia dumps). We follow Yu et al., (2017) and compute the BLEU between the generations and the test sets of both corpora by treating the test set as the references for each generation. The tests sets are large; we subsample 5000 examples from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(data_file, replacements={}, uncased=True):\n",
    "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
    "    if uncased:\n",
    "        data = [[t.lower() for t in sent] for sent in data]\n",
    "        \n",
    "    for k, v in replacements.items():\n",
    "        data = [[t if t != k else v for t in sent] for sent in data]\n",
    " \n",
    "    return data\n",
    "\n",
    "def prepare_wiki(data_file, uncased=True):\n",
    "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
    "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
    "\n",
    "def prepare_tbc(data_file):        \n",
    "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
    "    return prepare_data(data_file, replacements=replacements)\n",
    "\n",
    "def corpus_bleu(generated, references):\n",
    "    \"\"\" Compute similarity between two corpora as measured by\n",
    "    comparing each sentence of `generated` against all sentences in `references` \n",
    "    \n",
    "    args:\n",
    "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
    "        - references (List[List[str]]): list of sentences (split into tokens)\n",
    "        \n",
    "    returns:\n",
    "        - bleu (float)\n",
    "    \"\"\"    \n",
    "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki103_file = 'data/wiki103.5k.txt'\n",
    "tbc_file = 'data/tbc.5k.txt'\n",
    "\n",
    "wiki_data = prepare_wiki(wiki103_file)\n",
    "tbc_data = prepare_tbc(tbc_file)\n",
    "#sents = [detokenize(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"BERT-TBC BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data)))\n",
    "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, wiki_data)))\n",
    "print(\"BERT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data[:2500] + wiki_data[:2500])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to existing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI Generative Pretraining Transformer is another pretrained model successfully used for transfer learning. Since the model is a unidirectional language model, we can straightforwardly generate from the model. See [this repo](https://github.com/huggingface/pytorch-openai-transformer-lm) by Thomas Wolf at Huggingface for instructions for setting up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(\".\", \"pytorch-openai-transformer-lm\"))\n",
    "\n",
    "from model_pytorch import LMModel, load_openai_pretrained_model, DEFAULT_CONFIG\n",
    "from text_utils import TextEncoder\n",
    "\n",
    "def load_openai_gpt(n_special=1, n_ctx=512):\n",
    "    text_encoder = TextEncoder(\"/virtualmachines/models/pytorch-openai-transformer-lm/encoder_bpe_40000.json\", \n",
    "                               \"/virtualmachines/models/pytorch-openai-transformer-lm/vocab_40000.bpe\")\n",
    "    encoder = text_encoder.encoder\n",
    "    n_vocab = len(text_encoder.encoder)\n",
    "    vocab = n_vocab + n_special + n_ctx\n",
    "\n",
    "    args = DEFAULT_CONFIG\n",
    "    lm_model = LMModel(args, vocab, n_ctx, return_probs=True)\n",
    "    load_openai_pretrained_model(lm_model.transformer, n_ctx=n_ctx, n_special=n_special,\n",
    "                                 path=\"/virtualmachines/models/pytorch-openai-transformer-lm/\",\n",
    "                                 path_names=\"pytorch-openai-transformer-lm/\")\n",
    "    #lm_model.to(device)\n",
    "    lm_model.return_probs = False\n",
    "    lm_model.eval()\n",
    "    return lm_model, text_encoder\n",
    "\n",
    "def make_batch(X, n_vocab, n_special, batch_size):\n",
    "    X = np.array(X)\n",
    "    assert X.ndim in [1, 2]\n",
    "    if X.ndim == 1:\n",
    "        X = np.expand_dims(X, axis=0)\n",
    "    pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1])\n",
    "    pos_enc = np.tile(pos_enc, (batch_size, 1)) #np.expand_dims(pos_enc, axis=0)\n",
    "    batch = np.stack([X, pos_enc], axis=-1)\n",
    "    batch = torch.tensor(batch, dtype=torch.long)#.to(device)\n",
    "    return batch\n",
    "\n",
    "def append_batch(X, next_idx):\n",
    "    next_pos = X[:, -1:, 1] + 1\n",
    "    next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1)\n",
    "    return torch.cat((X, next_x), 1)\n",
    "\n",
    "def _generate_sentence_openai(model, text_encoder, seed_text, batch_size=10, gen_len=20, \n",
    "                             topk=100, sample=True, n_special=0):\n",
    "    n_vocab = len(text_encoder.encoder)\n",
    "    #X = np.random.randint(n_vocab, size=(batch_size, 1)).tolist()\n",
    "    #sents = [[text_encoder.decoder[X[i][0]]].replace('</w>', '') for i in range(batch_size)]\n",
    "    X = [[n_vocab - 1] for _ in range(batch_size)]\n",
    "    sents = [[] for _ in range(batch_size)]\n",
    "    if seed_text:\n",
    "        seed_ids = text_encoder.encode([seed_text,])\n",
    "        X = [X[i] + seed_ids[0] for i in range(batch_size)]\n",
    "        sents = [[seed_text] for _ in range(batch_size)]\n",
    "    XMB = make_batch(X, n_vocab, n_special, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    for step_n in range(gen_len):\n",
    "        out = model(XMB) + model.pos_emb_mask\n",
    "        next_idxs = generate_step(out, gen_idx=step_n, top_k=topk, sample=sample, return_list=False)\n",
    "        idxs = next_idxs.tolist()\n",
    "        for i in range(batch_size):\n",
    "            next_token = idxs[i]\n",
    "            if next_token == n_vocab:\n",
    "                next_token = \"<EOS>\"\n",
    "            else:\n",
    "                next_token = text_encoder.decoder[next_token].replace('</w>', '')\n",
    "            sents[i].append(next_token)\n",
    "        XMB = append_batch(XMB, next_idxs.unsqueeze(-1))\n",
    "        \n",
    "    return [[tok for tok in sent if tok != '\\n'] for sent in sents]\n",
    "\n",
    "def generate_openai(model, text_encoder, n_samples, seed_text, \n",
    "                    batch_size=10, gen_len=20, \n",
    "                    topk=100, temperature=temperature, sample=sample,\n",
    "                    n_special=0, print_every=1):\n",
    "    sents = []\n",
    "    start_time = time.time()\n",
    "    n_batches = math.ceil(n_samples / batch_size)\n",
    "    for batch_n in range(n_batches):\n",
    "        batch_sents = _generate_sentence_openai(model, text_encoder, seed_text,\n",
    "                                                batch_size=batch_size, gen_len=gen_len, \n",
    "                                                topk=topk, sample=sample,\n",
    "                                                n_special=n_special)\n",
    "        sents += batch_sents\n",
    "        if (batch_n + 1) % print_every == 0:\n",
    "            print(\"Generated batch %d of %d in %.3fs\" % (batch_n + 1, n_batches, time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpt_model, gpt_text_encoder = load_openai_gpt(n_special=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "batch_size = 5\n",
    "max_len = 15\n",
    "top_k = 5000\n",
    "temperature = 1\n",
    "\n",
    "sample = True\n",
    "\n",
    "openai_sents = generate_openai(gpt_model, gpt_text_encoder, seed_text=\"hello table chair\", \n",
    "                               n_samples=n_samples, batch_size=batch_size, gen_len=max_len,\n",
    "                               topk=top_k, temperature=temperature, sample=sample,\n",
    "                               n_special=1, print_every=1)\n",
    "openai_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printer(openai_sents[9], should_detokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"GPT-TBC BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data)))\n",
    "print(\"GPT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, wiki_data)))\n",
    "print(\"GPT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data[:2500] + wiki_data[:2500])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus as reference. Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def self_bleu(sents):\n",
    "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
    "\n",
    "def get_ngram_counts(sents, max_n=4):\n",
    "    size2count = {}\n",
    "    for i in range(1, max_n + 1):\n",
    "        size2count[i] = Counter([n for sent in sents for n in ngrams(sent, i)])\n",
    "    return size2count\n",
    "\n",
    "def ref_unique_ngrams(preds, refs, max_n=4):\n",
    "    # get # of *distinct* pred ngrams that don't appear in ref\n",
    "    pct_unique = {}\n",
    "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
    "    ref_ngrams = get_ngram_counts(refs, max_n)\n",
    "    for i in range(1, max_n + 1):\n",
    "        pred_ngram_counts = set(pred_ngrams[i].keys())\n",
    "        total = sum(pred_ngrams[i].values())\n",
    "        ref_ngram_counts = set(ref_ngrams[i].keys())\n",
    "        pct_unique[i] = len(pred_ngram_counts.difference(ref_ngram_counts)) / total\n",
    "    return pct_unique\n",
    "        \n",
    "def self_unique_ngrams(preds, max_n=4):\n",
    "    # get # of pred ngrams with count 1\n",
    "    pct_unique = {}\n",
    "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
    "    for i in range(1, max_n + 1):\n",
    "        n_unique = len([k for k, v in pred_ngrams[i].items() if v == 1])\n",
    "        total = sum(pred_ngrams[i].values())\n",
    "        pct_unique[i] = n_unique / total\n",
    "    return pct_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"BERT self-BLEU: %.2f\" % (100 * self_bleu(bert_sents)))\n",
    "print(\"OpenAI self-BLEU: %.2f\" % (100 * self_bleu(openai_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_n = 4\n",
    "\n",
    "pct_uniques = ref_unique_ngrams(bert_sents, wiki_data, max_n)\n",
    "for i in range(1, max_n + 1):\n",
    "    print(\"BERT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
    "pct_uniques = ref_unique_ngrams(bert_sents, tbc_data, max_n)\n",
    "for i in range(1, max_n + 1):\n",
    "    print(\"BERT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
    "pct_uniques = self_unique_ngrams(bert_sents, max_n)\n",
    "for i in range(1, max_n + 1):\n",
    "    print(\"BERT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pct_uniques = ref_unique_ngrams(openai_sents, wiki_data, max_n)\n",
    "for i in range(1, max_n + 1):\n",
    "    print(\"GPT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
    "pct_uniques = ref_unique_ngrams(openai_sents, tbc_data, max_n)\n",
    "for i in range(1, max_n + 1):\n",
    "    print(\"GPT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
    "pct_uniques = self_unique_ngrams(openai_sents, max_n)\n",
    "for i in range(1, max_n + 1):\n",
    "    print(\"GPT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
